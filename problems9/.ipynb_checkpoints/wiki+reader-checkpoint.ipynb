{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Задание по много поточности:\n",
    "\n",
    "Вам необходимо проанализировать википедию на предмет того, какие слова в каждой из частей речи встречаются чаще. Вы хотите реализовать это в несколько потоков.\n",
    "\n",
    "Запросы к википедии можно осуществлять с помощью библиотеки wikipedia. Для морфологического анализа используйте библиотеку pymorphy2. Чтобы разбить текст на слова можете воспользоваться функцией word_tokenize из библиотеки nltk.\n",
    "\n",
    "Класс должен иметь функции, приведенные ниже (но может иметь и другие на ваше усмотрение)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 1\n",
    "\n",
    "<b>Многопоточной реализация</b>\n",
    "\n",
    "Задачи делятся на три типа:\n",
    "<ul>\n",
    "<li><i>Получение данных</i>:\n",
    "<ol>\n",
    "<li>Получение заголовков для страниц википедии - запускает по max_threads функций, которые асинхронно получают заголовки страниц.</li>\n",
    "<li>Получение конкретных страниц - ждем, пока не появятся новые заголовки, которые не обработаны.\n",
    "Когда появились - начинаем запрашивать в max_threads функциях конкретные страницы по заголовкам.</li>\n",
    "</ol>\n",
    "</li>\n",
    "<li><i>Обработка данных</i>:\n",
    "<ol>\n",
    "<li>Ждем, пока не появятся новые необработанные страницы. Когда появляются, запускаем по max_threads функций для морфологического анализа слов.</li>\n",
    "<li>Ждем пока не появились обработанные слова. Как только появляется новое слово, сразу же обновляем _stats.</li>\n",
    "</ol>\n",
    "</li>\n",
    "<li><i>Сохранение данных</i>:\n",
    "<ol>\n",
    "        Раз в store_every обработанных слов вызывается асинхронно функция dump, которая сохраняет _stats.\n",
    "</ol>\n",
    "</li>\n",
    "</ul>\n",
    "<b>P. S.</b>\n",
    "\n",
    "Комментарии специально запутанные, чтобы вы сами придумали архитектуру вызова потоков. Не бойтесь использовать Queue и daemon=True. Запрещается использовать threading.Lock / threading.RLock или другие блокировки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia as wiki\n",
    "import pymorphy2\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from queue import LifoQueue\n",
    "from time import time\n",
    "\n",
    "import threading as td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiReader(object):\n",
    "    \"\"\"\n",
    "    Класс для работы с википедией.\n",
    "    Собирает статисткику по словам каждой части речи в статьях википедии.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    morphs: list\n",
    "        Части речи, которые хотим исследовать. Слова другой части речи не включаются в статистику.\n",
    "    \n",
    "    page_per_req: int\n",
    "        Количество случайных названий страниц, запрашиваемых за один раз у википедии.\n",
    "    \n",
    "    max_threads_per_job: int\n",
    "        Количество потоков, запускаемых другим потоком демоном (можно не использовать, если получится).\n",
    "    \n",
    "    max_words: int\n",
    "        Количество слов для обработки.\n",
    "    \n",
    "    store_every: int\n",
    "        Как часто сохранять данные на диск. Каждые store_every слов вызывается функция dump.\n",
    "    \n",
    "    store_path: str\n",
    "        Куда сохранять данные.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    _stats: <your code here>\n",
    "        Структура данных (возможно встроенная), позволяющая хранить для каждой части речи список слов с их количеством.\n",
    "        Необходимо, чтобы получение (изменение) статистики (текущего количества) для каждой пары\n",
    "        <часть речи, слово> происходило за O(1).\n",
    "            \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 morphs=['NOUN', 'ADJF', 'VERB'],\n",
    "                 page_per_req=4,\n",
    "                 max_threads_per_job=2,\n",
    "                 max_words=10000,\n",
    "                 store_every=1000,\n",
    "                 store_path=\"stats.txt\"):\n",
    "        self._stats = Counter()\n",
    "        \n",
    "        self._titles = LifoQueue()\n",
    "        self._pages = LifoQueue()\n",
    "        self._words = LifoQueue()\n",
    "        \n",
    "        self._morph = pymorphy2.MorphAnalyzer()\n",
    "        self._morphs = morphs\n",
    "        \n",
    "        self._page_per_req = page_per_req\n",
    "        self._max_threads_per_job = max_threads_per_job\n",
    "        self._max_words = max_words\n",
    "        \n",
    "        self._store_every = store_every\n",
    "        self._store_path = store_path\n",
    "        \n",
    "        wiki.set_lang('ru')\n",
    "    \n",
    "    def run(self):        \n",
    "        self._words_count = 0\n",
    "        self._stop = False\n",
    "        \n",
    "        title_gens = [td.Thread(target=self._gen_titles, daemon=True) for _ in range(self._max_threads_per_job)]\n",
    "        page_gens = [td.Thread(target=self._gen_pages, daemon=True) for _ in range(self._max_threads_per_job)]\n",
    "        word_gens = [td.Thread(target=self._gen_words, daemon=True) for _ in range(self._max_threads_per_job)]\n",
    "        word_procs = [td.Thread(target=self._process_words, daemon=True) for _ in range(self._max_threads_per_job)]\n",
    "        \n",
    "        print('starting', self._max_threads_per_job * 4, 'threads')\n",
    "        \n",
    "        start = time()\n",
    "        \n",
    "        for thread in title_gens:\n",
    "            thread.start()\n",
    "            \n",
    "        for thread in page_gens:\n",
    "            thread.start()\n",
    "            \n",
    "        for thread in word_gens:\n",
    "            thread.start()\n",
    "            \n",
    "        for thread in word_procs:\n",
    "            thread.start()\n",
    "                \n",
    "        print('waiting for threads:\\ntitle jobs: ', end='')\n",
    "                \n",
    "        for thread in title_gens:\n",
    "            thread.join()\n",
    "            print('.', end='')\n",
    "        print(' terminated\\npage jobs: ', end='')\n",
    "            \n",
    "        for thread in page_gens:\n",
    "            thread.join()\n",
    "            print('.', end='')\n",
    "        print(' terminated\\nword jobs: ', end='')\n",
    "            \n",
    "        for thread in word_gens:\n",
    "            thread.join()\n",
    "            print('.', end='')\n",
    "        print(' terminated\\nprocessing jobs: ', end='')\n",
    "        \n",
    "        for thread in word_procs:\n",
    "            thread.join()\n",
    "            print('.', end='')\n",
    "        print(' terminated')\n",
    "        \n",
    "        start = time() - start\n",
    "        print('wall time: {}s'.format(start))\n",
    "        \n",
    "    def dump(self, path=None):\n",
    "        if path is None:\n",
    "            fout = open(self._store_path, 'w')\n",
    "        else:\n",
    "            fout = open(path, 'w')\n",
    "        \n",
    "        for i in self._stats:\n",
    "            print('{}: {}'.format(i, self._stats[i]), file=fout)\n",
    "        \n",
    "        fout.close()\n",
    "    \n",
    "    def load(self, path=None):\n",
    "        if path is None:\n",
    "            fin = open(self._store_path)\n",
    "        else:\n",
    "            fin = open(path)\n",
    "            \n",
    "        cnt = {}\n",
    "        for i in fin.readlines():\n",
    "            k, v = i.split()\n",
    "            cnt[k] = v\n",
    "            \n",
    "        fin.close()\n",
    "        \n",
    "        return cnt\n",
    "    \n",
    "    def _get_word_grammeme(self, word):\n",
    "        return self._morph.parse(word)[0].tag.POS\n",
    "    \n",
    "    def _gen_titles(self):\n",
    "        while not self._stop:\n",
    "            titles = wiki.random(pages=self._page_per_req)\n",
    "            for title in titles:\n",
    "                self._titles.put(title)\n",
    "        \n",
    "    def _gen_pages(self):\n",
    "        while not self._stop:\n",
    "            title = self._titles.get()\n",
    "            self._titles.task_done()\n",
    "            \n",
    "            try:\n",
    "                page = wiki.page(title=title)\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            self._pages.put(page)\n",
    "            \n",
    "    def _gen_words(self):\n",
    "        while not self._stop:\n",
    "            page = self._pages.get()\n",
    "            self._pages.task_done()\n",
    "            \n",
    "            for word in word_tokenize(page.content):\n",
    "                self._words.put(word)\n",
    "                \n",
    "    def _process_words(self):\n",
    "        global start\n",
    "        while not self._stop:\n",
    "            word = self._words.get()\n",
    "            if word is not None:\n",
    "                self._words_count += 1\n",
    "                if self._words_count % self._store_every == 0:\n",
    "                    self.dump()\n",
    "                    \n",
    "                if self._words_count == self._max_words:\n",
    "                    self._stop = True\n",
    "                    \n",
    "                gram = self._get_word_grammeme(word)\n",
    "                if gram not in self._morphs:\n",
    "                    self._stats['REST'] += 1\n",
    "                else:\n",
    "                    self._stats[gram] += 1\n",
    "                    \n",
    "            self._words.task_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting 8 threads\n",
      "waiting for threads:\n",
      "title jobs: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 884 of the file C:\\Program Files\\Anaconda3\\lib\\threading.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. terminated\n",
      "page jobs: .. terminated\n",
      "word jobs: .. terminated\n",
      "processing jobs: .. terminated\n",
      "wall time: 17.270789861679077s\n"
     ]
    }
   ],
   "source": [
    "reader = WikiReader(max_threads_per_job=1)\n",
    "reader.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REST: 5245\n",
      "NOUN: 3162\n",
      "ADJF: 1058\n",
      "VERB: 506\n"
     ]
    }
   ],
   "source": [
    "cnt = reader.load()\n",
    "for i in cnt.items():\n",
    "    print(*i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting 4 threads\n",
      "waiting for threads:\n",
      "title jobs: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 884 of the file C:\\Program Files\\Anaconda3\\lib\\threading.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". terminated\n",
      "page jobs: . terminated\n",
      "word jobs: . terminated\n",
      "processing jobs: . terminated\n",
      "wall time: 34.072606563568115s\n"
     ]
    }
   ],
   "source": [
    "reader = WikiReader(max_threads_per_job=2)\n",
    "reader.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REST: 5028\n",
      "NOUN: 3330\n",
      "ADJF: 1064\n",
      "VERB: 577\n"
     ]
    }
   ],
   "source": [
    "cnt = reader.load()\n",
    "for i in cnt.items():\n",
    "    print(*i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
